# 机器学习之线性回归

## 1. 矩阵形式推导
对于输入$X$，共有m个样本，每个样本有n个特征，权重$\theta $共有n个参数(包含偏置)，真实输出$y$只含单特征

$$
X={\left[
\begin{matrix}
x^{(1)}       \\
x^{(2)}       \\
\vdots        \\
 x^{(m)}      \\
\end{matrix}
\right]}={\left[
\begin{matrix}
 x^{(1)}_0      & x^{(1)}_1      & \cdots & x^{(1)}_n      \\
 x^{(2)}_0      & x^{(2)}_1      & \cdots & x^{(2)}_n      \\
 \vdots & \ddots & \vdots \\
 x^{(m)}_0      & x^{(m)}_1      & \cdots & x^{(m)}_n      \\
\end{matrix}
\right]}_{m \times n} \theta = {\left[
\begin{matrix}
\theta _{0}       \\
\theta _{1}       \\
\vdots        \\
\theta _{n}      \\
\end{matrix}
\right]}_{n\times1} y={\left[
\begin{matrix}
y^{(1)}       \\
y^{(2)}       \\
\vdots        \\
y^{(m)}      \\
\end{matrix}
\right]}_{m\times1} 
$$
预测值$\hat{y}$为
$$
\hat{y}=X\cdot\theta={\left[
\begin{matrix}
 x^{(1)}_0      & x^{(1)}_1      & \cdots & x^{(1)}_n      \\
 x^{(2)}_0      & x^{(2)}_1      & \cdots & x^{(2)}_n      \\
 \vdots & \ddots & \vdots \\
 x^{(m)}_0      & x^{(m)}_1      & \cdots & x^{(m)}_n      \\
\end{matrix}
\right]}_{m \times n}{\left[
\begin{matrix}
\theta _{0}       \\
\theta _{1}       \\
\vdots        \\
\theta _{n}      \\
\end{matrix}
\right]}_{n\times1} 
$$
$$
= {\left[
\begin{matrix}
\theta _{1}\cdot x^{(1)}_0+\theta _{1}\cdot x^{(1)}_1+\cdots  +\theta _n\cdot x^{(1)}_n    \\
\theta _{1}\cdot x^{(2)}_0+\theta _{1}\cdot x^{(2)}_1+\cdots  +\theta _n\cdot x^{(2)}_n    \\
\vdots        \\
\theta _{1}\cdot x^{(m)}_0+\theta _{1}\cdot x^{(m)}_1+\cdots  +\theta _n\cdot x^{(m)}_n    \\
\end{matrix}
\right]}_{m\times1} = {\left[
\begin{matrix}
\hat y^{(1)}       \\
\hat y^{(2)}       \\
\vdots        \\
\hat y^{(m)}      \\
\end{matrix}
\right]}_{m\times1} 
$$

误差$E$为
$$
E = \hat y - y = {\left[
\begin{matrix}
\hat y^{(1)}       \\
\hat y^{(2)}       \\
\vdots        \\
\hat y^{(m)}      \\
\end{matrix}
\right]}_{m\times1}  - {\left[
\begin{matrix}
 y^{(1)}       \\
y^{(2)}       \\
\vdots        \\
y^{(m)}      \\
\end{matrix}
\right]}_{m\times1} = {\left[
\begin{matrix}
\hat y^{(1)}-y^{(1)}       \\
\hat y^{(2)}-y^{(2)}       \\
\vdots        \\
\hat y^{(m)}- y^{(m)}      \\
\end{matrix}
\right]}_{m\times1}  
$$
参数更新
$$
\theta = \theta - \alpha \cdot X^T \cdot E = {\left[
\begin{matrix}
\theta _{0}       \\
\theta _{1}       \\
\vdots        \\
\theta _{n}      \\
\end{matrix}
\right]}_{n\times1}-\alpha \cdot  {\left[
\begin{matrix}
 x^{(1)}_0      & x^{(2)}_0      & \cdots & x^{(m)}_0     \\
 x^{1)}_1      & x^{(2)}_1      & \cdots & x^{(m)}_1      \\
 \vdots & \ddots & \vdots \\
 x^{(1)}_n      & x^{(2)}_n      & \cdots & x^{(m)}_n      \\
\end{matrix}
\right]}_{n \times m} {\left[
\begin{matrix}
\hat y^{(1)}-y^{(1)}       \\
\hat y^{(2)}-y^{(2)}       \\
\vdots        \\
\hat y^{(m)}- y^{(m)}      \\
\end{matrix}
\right]}_{m\times1}  
$$
$$
={\left[
\begin{matrix}
\theta _{0}       \\
\theta _{1}       \\
\vdots        \\
\theta _{n}      \\
\end{matrix}
\right]}_{n\times1}-\alpha \cdot  {\left[
\begin{matrix}
 \sum_{i=1}^m  (\hat y^{(i)} -y^{(i)})\cdot X^{(i)}_0\\
 \sum_{i=1}^m  (\hat y^{(i)} -y^{(i)})\cdot X^{(i)}_1\\
 \vdots \\
 \sum_{i=1}^m  (\hat y^{(i)} -y^{(i)})\cdot X^{(i)}_n      \\
\end{matrix}
\right]}_{n \times 1}
={\left[
\begin{matrix}
 \theta_0 - \sum_{i=1}^m  (\hat y^{(i)} -y^{(i)})\cdot X^{(i)}_0\\
 \theta_1 - \sum_{i=1}^m  (\hat y^{(i)} -y^{(i)})\cdot X^{(i)}_1\\
 \vdots \\
 \theta_n - \sum_{i=1}^m  (\hat y^{(i)} -y^{(i)})\cdot X^{(i)}_n      \\
\end{matrix}
\right]}_{n \times 1}
$$

## 2. python实现
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


class LinearRegression(object):
    def __init__(self):
        self.theta = None
        self.loss_list = []

    def fit(self, X, y, lr=0.001, iters=1000):
        """train the model with input X and y"""
        # add bias if X without bias
        if np.sum(X[:, -1] - np.ones(X.shape[0])) != 0:
            X = np.hstack((X, np.ones((X.shape[0], 1))))
        self.X = X
        self.y = y
        self.sample_num = self.X.shape[0]
        self.feature_num = self.X.shape[1] - 1
        theta = np.random.randn(self.X.shape[1], 1)

        for i in range(iters):
            # compute and record the loss
            y_pred = X @ theta
            error = y_pred - y 
            loss = np.sum(error**2) / (2 * self.sample_num)
            print("At iter {0}, loss = {1}".format(i + 1, loss))
            self.loss_list.append(loss)

            # upgrade theta through gradient descent
            grad = X.T @ error / self.sample_num
            theta = theta - lr * grad

        # record the final theta
        self.theta = theta
        print("Final theta: {0}".format(theta))

    def predict(self, X):
        return X @ self.theta

    def plot_data(self):
        """plot the data distribute and the decision plane"""
        fig = plt.figure()
        if self.feature_num == 1:
            ax = fig.add_subplot(111)
            ax.scatter(self.X[:, 0], self.y[:, 0])
            x_ = np.array([self.X.min(), self.X.max()])
            y_ = self.theta[0] * x_ + self.theta[1]
            ax.plot(x_, y_, label="decision plane", c='r')
            plt.title("Data distribution")
            plt.xlabel("x")
            plt.ylabel("y")
            plt.legend()
            plt.show()
        elif self.feature_num == 2:
            ax = fig.add_subplot(111, projection='3d')
            ax.scatter(self.X[:, 0], self.X[:, 1], self.y[:, 0])
            x_ = np.linspace(self.X[:, 0].min(), self.X[:, 0].max(), 100)
            y_ = np.linspace(self.X[:, 1].min(), self.X[:, 1].max(), 100)
            x_, y_ = np.meshgrid(x_, y_)
            z_ = self.theta[0, 0] * x_ + self.theta[1, 0] * y_
            ax.plot_surface(x_, y_, z_)
            plt.show()

        else:
            print("unable to show data in high dimentional space")

    def plot_loss(self):
        fig = plt.figure()
        ax = fig.add_subplot(111)
        x_ = range(len(self.loss_list))
        y_ = self.loss_list
        ax.plot(x_, y_)
        plt.xlabel("iters")
        plt.ylabel("loss")
        plt.show()



if __name__ == "__main__":
    # read data from txt
    data = pd.read_csv('./ex1data1.txt', header=None)
    #data = pd.read_csv('./ex1data2.txt', header=None)
    data = (data - data.mean()) / data.std()
    data = data.values
    X = data[:, :-1]
    y = data[:, -1:]
    print(X.shape)
    print(y.shape)

    # train model
    lg = LinearRegression()
    lg.fit(X, y, lr=0.01, iters=1000)
    lg.plot_data()
    lg.plot_loss()
```