# 机器学习之激活函数
---
## 1. 激活函数的作用
* 对数据进行归一化，即将计算结果限制在某个范围内。
* 打破线性映射关系，为深层网络的实现提供了可能。

## 2. 常见激活函数
### sigmoid函数
**函数形式**：$$f(x)=\frac{1}{1+e^{-x}}$$
**反向求导**：$$\frac{\partial f(x)}{\partial x}=f(x)(1-f(x))$$
**图像**：![ipjuz6.png](https://s1.ax1x.com/2018/09/05/ipjuz6.png)
**优点**：
* 平滑曲线，连续可导
* 能作为二分类问题的输出层，输出值$y\in (0,1)$可以代表属于某一类的概率
* 会将输入限定在范围$(0,1)$内，不易出现数据不稳定现象  

**缺点**：
* 对sigmoid反向求导时，其最大值才0.25，收敛速度慢，对于深层网络会引起梯度消失，最终导致模型训练不足
* 存在指数操作，计算量大且易数值溢出
**python实现**：
```python
class Sigmoid(object):
	def __init__(self):
		self.name = 'sigmoid'
	def forward(self, x):
		self.input = x.copy()
		self.output = 1./(1.+np.exp(-x))
		return self.output
	def backward(self):
		return self.output * (1 - self.output)
```

### tanh函数
**函数形式**：$$f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$$
**反向求导**：$$\frac{\partial f(x)}{\partial x}=1-f(x)^2$$ 
**图像**：![ipjnRx.png](https://s1.ax1x.com/2018/09/05/ipjnRx.png)
**优点**：
* 0均值输出
* 收敛速度比sigmoid更快

**缺点**：
* 还是没有改变Sigmoid函数的最大问题——由于饱和性产生的梯度消失。
* 存在指数操作，计算量大且易数值溢出

**python实现**：
```python
class Tanh(object):
	def __init__(self):
		self.name = 'tanh'
	def forward(self, x):
		self.input = x.copy()
		self.output =(np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))
		return self.output
	def backward(self):
		return 1 - self.output ** 2
```


### ReLU(Rectified Linear Unit)函数
**函数形式**：$$f(x)=max(0,x)$$
**反向求导**：$$\frac{\partial f(x)}{\partial x}=\begin{cases} 
1 ,    && x>0 \\
0 ,    && x<0 \\
\end{cases}$$

**图像**：![ipjMQK.png](https://s1.ax1x.com/2018/09/05/ipjMQK.png)
**优点**：
* 算法简单(Simple is better than complex),收敛快
* 对正向接受域友好，不易损失梯度，优化效果好
* 单边抑制， 更宽广的接受域$(0,+\infty )$
* 具有线性性质，相当于完成一次向量投影(左乘一个非0即1的对角阵)，并舍弃其负数部分，这样的线性性质便于分析深层模型理论
$$relu(
 \left[
 \begin{matrix}
   0.4 \\
   0.2 \\
   -0.4
  \end{matrix}
  \right])=
 \left[
 \begin{matrix}
   1 & 0 & 0 \\
   0 & 1 & 0 \\
   0 & 0 & 0
  \end{matrix}
  \right]  \left[
 \begin{matrix}
   0.4 \\
   0.2 \\
   -0.4
  \end{matrix}
  \right]= \left[
 \begin{matrix}
   0.4 \\
   0.2 \\
   0
  \end{matrix}
  \right])
$$   

**缺点**：
* 过于宽广的接受域会导致数据的不稳定(解决方法：使用ReLU6、在ReLU前增加Batch Normalization、xavier初始化)
* 对负数的完全抑制导致部分神经元坏死，即Dying ReLU problem(解决方法：Leaky ReLU、pReLU)

**python实现**：
```python
class Relu(object):
	def __init__(self):
		self.name = 'relu'
	def forward(self, x):
		self.input = x.copy()
		self.output = (np.abs(x) + x)/2
		return self.output
	def backward(self):
		self.input[self.input>0]=1
		self.input[self.input<0]=0
		return self.input
```
## 3. 补充
### softplus
**函数形式**：
$$f(x)=\log (1+e^x)$$
**改进**：是relu的平滑版，其导数为sigmoid
### ReLU6
**函数形式**：$$f(x)=min(6,\frac{{\vert x\vert}+x}{2})$$
**改进**：基于经验值对输出的上界做了限定，防止数据不稳定

### Leaky ReLU
**函数形式**：$$f(x)=\begin{cases} 
x ,    && x>0 \\
\frac{x}{a},    && x<0 \\
\end{cases}$$
**改进**：对负数部分增加了一个固定斜率，使负数部分仍能得到更新，$a_i\in (1,+\infty)$

### pReLU(Parametric Rectified Linear Unit)
**函数形式**：$$f(x)=\begin{cases} 
x ,    && x>0 \\
\frac{x}{a_i} ,    && x<0 \\
\end{cases}$$
**改进**：斜率$a_i$通过带动量的方法更新，增加了部分计算量

### rReLU(Randomized Rectified Linear Unit)
$$f(x_{ij})=\begin{cases} 
x_{ij} ,    && x_{ij}>0 \\
\frac{x_{ij}}{a_{ij}} ,    && x_{ij}<0 \\
\end{cases}$$
**改进**：$a_{ij}$在给定的范围内随机抽取的值，这个值在测试环节就会固定下来。
