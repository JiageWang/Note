# 深度学习参数初始化
---
## Xavier
**方法概述**：对于输入维度为n，输出维度为m的层，其参数从$\left[-\sqrt{\frac{6}{m+n}}, \sqrt{{\frac{6}{m+n}}}   \right ] $ 均匀分布内选取

**数学推导**：
* 三个假设：
1. 忽略偏置项的影响
2. 激活函数使用tanh(在0附近导数为1),传播过程中可忽略不计
3. 输入数据与参数相互独立
* 统计学基础知识：
随机变量$s$和$w$，各自服从均值为0，方差为$\sigma$的分布,则：
$wx$服从均值为0，方差为$\sigma^2$的分布
$wx+wx$服从均值为0，方差为$2\sigma^2$的分布

假设卷积层输入为$x$，输出为$z$，卷积核参数为$w$，共$n=c*h*w$个参数，其计算公式如下
$$z_j = \sum_i^n w_i \cdot x_i$$
1).正向传播
该层的输出
$$\sigma^{k+1}_x=n^k\cdot\sigma^k_x\cdot\sigma^k_w$$
于是对于k层网络，总输出
$$\sigma^k_x=\sigma^1_x\prod_{i=1}^{k-1}n^i\cdot\sigma_w^i$$
连乘项$n^i\cdot\sigma_w^i$若总大于一，则数值幅度越来越大，已造成数据不稳定；若其总小于1则会造成模型退化问题。
由$\sigma^{k+1}_x=n^k\cdot\sigma^k_x\cdot\sigma^k_w$，欲使$\sigma^{k+1}_x=\sigma^{k}_x$，只需
$$\sigma^{k}_w = \frac{1}{n^k}$$
即只需用均值为0，方差为$\frac{1}{n^k}$的分布去初始化第$k$层的参数
2).反向传播
网络对$k-1$层的导数有
$$\frac{\partial L}{\partial x^{k-1}_{j}}=\sum_{i=1}^m \frac{\partial L}{\partial x^k_i}\cdot w^k_j$$
其中m表示第k-1层的输出维度，使用$\nabla x^k_j$表示第k层第j个元素的梯度，则
$$\sigma{(\nabla x^{k-1}_j)}=m^k\cdot \sigma{(\nabla x^{k}_i)}\cdot \sigma^k_w$$
同理可得，欲使$\sigma{(\nabla x^{k-1}_j)}=\sigma{(\nabla x^{k}_i)}$，只需
$$\sigma^{k}_w = \frac{1}{m^k}$$

综合可得
$$\sigma^{k}_w = \frac{2}{n^k+m^k}$$
均匀分布$[a,b]$的方差：
$$\sigma = \frac{(b-a)^2}{12}$$
假设初始化范围为$[-a,a]$
$$\frac{(a-(-a))^2}{12}=\frac{a^2}{3}=\sigma^k_w$$
将$a^k_w$带入可得
$$a=\sqrt \frac{6}{n+m}$$






